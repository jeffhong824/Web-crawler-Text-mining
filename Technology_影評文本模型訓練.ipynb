{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尋找Negative"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "《目前訓練資料來源》\n",
    "\n",
    "IMDB 影評資料\n",
    "\n",
    "### kaggle的Bag of Words Meets Bags of Popcorn (主要是有50,000則標籤過的 IMDB的電影評論)\n",
    "#### 情緒分為 0 和 1\n",
    "#### - 評分大於 7 的為 1\n",
    "#### - 評分小於 5 的為 0\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算訓練資料的字句最大字數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len  42\n",
      "nb_words  2268\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import GRU,SimpleRNN, Activation, Dense,Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "data_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/NLP/查詢序列/影評正負面模型/Sentiment1_training.txt'\n",
    "with open(data_path,'r+', encoding='UTF-8') as f: #影評 NLTK Sentiment1_training   train_mood\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        num_recs += 1\n",
    "print('max_len ',maxlen)\n",
    "print('nb_words ', len(word_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備數據 onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 準備數據\n",
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word_index = {x[0]: i+2 for i, x in enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word_index[\"PAD\"] = 0\n",
    "word_index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word_index.items()}\n",
    "X = np.empty(num_recs,dtype=list)\n",
    "y = np.zeros(num_recs)\n",
    "i=0\n",
    "data_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/NLP/查詢序列/影評正負面模型/Sentiment1_training.txt'\n",
    "# 讀取訓練資料，將每一單字以 dictionary 儲存\n",
    "with open(data_path,'r+', encoding='UTF-8') as f: #Sentiment1_training.txt train_mood.txt\n",
    "    for line in f:\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        seqs = []\n",
    "        for word in words:\n",
    "            if word in word_index:\n",
    "                seqs.append(word_index[word])\n",
    "            else:\n",
    "                seqs.append(word_index[\"UNK\"])\n",
    "        X[i] = seqs\n",
    "        y[i] = int(label)\n",
    "        i += 1\n",
    "# 字句長度不足補空白        \n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/NLP/查詢序列/影評正負面模型/模型/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實測副函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mood(text,model_path,data_path):\n",
    "\n",
    "    Text1=input(text)\n",
    "#     print(Text1,end='')\n",
    "\n",
    "    #print(\"\\n predict ......\")\n",
    "    model= load_model(model_path)\n",
    "\n",
    "    ## 探索數據分析(EDA)\n",
    "    # 計算訓練資料的字句最大字數\n",
    "    maxlen = 0\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs = 0\n",
    "    with open(data_path,'r+', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            label, sentence = line.strip().split(\"\\t\")\n",
    "            words = nltk.word_tokenize(sentence.lower())\n",
    "            if len(words) > maxlen:\n",
    "                maxlen = len(words)\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "            num_recs += 1\n",
    "\n",
    "    ## 準備數據\n",
    "    MAX_FEATURES = 2000\n",
    "    MAX_SENTENCE_LENGTH = 40\n",
    "    vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "    word_index = {x[0]: i+2 for i, x in enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "    word_index[\"PAD\"] = 0\n",
    "    word_index[\"UNK\"] = 1\n",
    "    index2word = {v:k for k, v in word_index.items()}\n",
    "    X = np.empty(num_recs,dtype=list)\n",
    "    y = np.zeros(num_recs)\n",
    "    i=0\n",
    "    # 讀取訓練資料，將每一單字以 dictionary 儲存\n",
    "    with open(data_path,'r+', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            label, sentence = line.strip().split(\"\\t\")\n",
    "            words = nltk.word_tokenize(sentence.lower())\n",
    "            seqs = []\n",
    "            for word in words:\n",
    "                if word in word_index:\n",
    "                    seqs.append(word_index[word])\n",
    "                else:\n",
    "                    seqs.append(word_index[\"UNK\"])\n",
    "            X[i] = seqs\n",
    "            y[i] = int(label)\n",
    "            i += 1\n",
    "\n",
    "    # 字句長度不足補空白        \n",
    "    X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n",
    "\n",
    "\n",
    "    INPUT_SENTENCES = [Text1]# ,Text2,Text3\n",
    "    XX = np.empty(len(INPUT_SENTENCES),dtype=list)\n",
    "    # 轉換文字為數值\n",
    "    i=0\n",
    "    for sentence in  INPUT_SENTENCES:\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        seq = []\n",
    "        for word in words:\n",
    "            if word in word_index:\n",
    "                seq.append(word_index[word])\n",
    "            else:\n",
    "                seq.append(word_index['UNK'])\n",
    "        XX[i] = seq\n",
    "        i+=1\n",
    "\n",
    "    XX = sequence.pad_sequences(XX, maxlen=MAX_SENTENCE_LENGTH)\n",
    "    # 預測，並將結果四捨五入，轉換為 0 或 1\n",
    "    labels = [int(round(x[0])) for x in model.predict(XX) ]\n",
    "    label2word = {1:'正面', 0:'負面'}\n",
    "    # 顯示結果\n",
    "\n",
    "    #print(\"\\n以下分析剛剛對話情緒:\")\n",
    "\n",
    "    for i in range(len(INPUT_SENTENCES)):\n",
    "        print(''+label2word[labels[i]]) #, INPUT_SENTENCES[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料劃分訓練組及測試組\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 2s 266us/step - loss: 0.2107 - acc: 0.9086 - val_loss: 0.1122 - val_acc: 0.9478\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 1s 175us/step - loss: 0.0308 - acc: 0.9903 - val_loss: 0.0885 - val_acc: 0.9654\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 1s 161us/step - loss: 0.0105 - acc: 0.9988 - val_loss: 0.0728 - val_acc: 0.9746\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 1s 167us/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0739 - val_acc: 0.9774\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 1s 165us/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0878 - val_acc: 0.9711\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 1s 169us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0720 - val_acc: 0.9788\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 1s 164us/step - loss: 9.9686e-04 - acc: 0.9998 - val_loss: 0.0880 - val_acc: 0.9725\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 1s 166us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0908 - val_acc: 0.9739\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 1s 169us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.1669 - val_acc: 0.9528\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 1s 163us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0748 - val_acc: 0.9774\n",
      "1418/1418 [==============================] - 0s 37us/step\n",
      "\n",
      " accuracy: 0.977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97       626\n",
      "         1.0       0.98      0.98      0.98       792\n",
      "\n",
      "    accuracy                           0.98      1418\n",
      "   macro avg       0.98      0.98      0.98      1418\n",
      "weighted avg       0.98      0.98      0.98      1418\n",
      "\n",
      "預測   真實      句子\n",
      " 0      0     as i sit here , watching the mtv movie awards , i am reminded of how much i despised the movie brokeback mountain .\n",
      " 1      1     just because the da vinci code is awesome does n't mean jesus ca n't be the most admired bachelor ! .\n",
      " 0      0     i hate harry potter with a passion but these are UNK ! UNK\n",
      " 1      1     which is why i said silent hill turned into reality coz i was hella like goin mission impossible down that bitch .\n",
      " 0      0     da vinci code sucks .\n"
     ]
    }
   ],
   "source": [
    "# 模型構建\n",
    "\n",
    "model = Sequential()\n",
    "# 加『嵌入』層\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "# 加『LSTM』層\n",
    "model.add(SimpleRNN(batch_input_shape=(None, word_freqs), units= 50,unroll=True,)) \n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# binary_crossentropy:二分法\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# 模型訓練\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))\n",
    "\n",
    "# 預測\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "predictions = model.predict_classes(Xtest)\n",
    "\n",
    "print(\"\\n accuracy: %.3f\" % (acc))\n",
    "print()\n",
    "print(classification_report(ytest, predictions))\n",
    "\n",
    "print('{}   {}      {}'.format('預測','真實','句子'))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))\n",
    "    \n",
    "\n",
    "model.save(Save_path+\"Mood_RNN_twitter.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy: 0.977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97       626\n",
      "         1.0       0.98      0.98      0.98       792\n",
      "\n",
      "    accuracy                           0.98      1418\n",
      "   macro avg       0.98      0.98      0.98      1418\n",
      "weighted avg       0.98      0.98      0.98      1418\n",
      "\n",
      "預測   真實      句子\n",
      " 0      0     brokeback mountain was boring .\n",
      " 0      0     always knows what i want , not guy crazy , hates harry potter ..\n",
      " 0      0     my dad 's being stupid about brokeback mountain ...\n",
      " 1      1     mission impossible 2 rocks ! ! ....\n",
      " 1      1     the story of harry potter is a deep and profound one , and i love harry potter .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n accuracy: %.3f\" % (acc))\n",
    "print()\n",
    "print(classification_report(ytest, predictions))\n",
    "\n",
    "print('{}   {}      {}'.format('預測','真實','句子'))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input I am good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am good  正面\n"
     ]
    }
   ],
   "source": [
    "mood(\"Input\",Save_path+\"Mood_RNN_twitter.h5\",data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.1818 - acc: 0.9174 - val_loss: 0.0521 - val_acc: 0.9803\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 2s 386us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0431 - val_acc: 0.9880\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 2s 349us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0485 - val_acc: 0.9901\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 2s 362us/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.0459 - val_acc: 0.9880\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 2s 355us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0468 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 2s 352us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0511 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 2s 353us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0547 - val_acc: 0.9873\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 2s 359us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0513 - val_acc: 0.9880\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 2s 362us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0519 - val_acc: 0.9908\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 2s 352us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0489 - val_acc: 0.9894\n",
      "1418/1418 [==============================] - 0s 79us/step\n",
      "\n",
      " accuracy: 0.989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       626\n",
      "         1.0       0.99      0.99      0.99       792\n",
      "\n",
      "    accuracy                           0.99      1418\n",
      "   macro avg       0.99      0.99      0.99      1418\n",
      "weighted avg       0.99      0.99      0.99      1418\n",
      "\n",
      " 1      1     harry potter is awesome i do n't care if anyone says differently ! ..\n",
      " 1      1     i 'm gayer than a picnic basket , and i love brokeback mountain , but crash was in my opinion the best movie of last year .\n",
      " 1      1     i either love brokeback mountain or think it 's great that homosexuality is becoming more acceptable ! :\n",
      " 0      0     who else thought the da vinci code kinda sucked ...\n",
      " 1      1     man i loved brokeback mountain !\n"
     ]
    }
   ],
   "source": [
    "# 模型構建\n",
    "\n",
    "model = Sequential()\n",
    "# 加『嵌入』層\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "# 加『LSTM』層\n",
    "model.add(GRU(batch_input_shape=(None, word_freqs), units= 50,unroll=True,)) \n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# binary_crossentropy:二分法\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# 模型訓練\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))\n",
    "\n",
    "# 預測\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "predictions = model.predict_classes(Xtest)\n",
    "\n",
    "print(\"\\n accuracy: %.3f\" % ( acc))\n",
    "print()\n",
    "print(classification_report(ytest, predictions))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))\n",
    "    \n",
    "\n",
    "model.save(Save_path+\"Mood_GRU_twitter.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input I hate u\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate u  負面\n"
     ]
    }
   ],
   "source": [
    "mood(\"Input\",Save_path+\"Mood_GRU_twitter.h5\",data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:127: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.2195 - acc: 0.9141 - val_loss: 0.0515 - val_acc: 0.9781\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 4s 690us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0512 - val_acc: 0.9859\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 4s 710us/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0664 - val_acc: 0.9753\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 4s 699us/step - loss: 0.0056 - acc: 0.9977 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 4s 703us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0493 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 4s 701us/step - loss: 9.6567e-04 - acc: 0.9998 - val_loss: 0.0543 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 4s 749us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0521 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 4s 731us/step - loss: 7.6971e-04 - acc: 0.9998 - val_loss: 0.0551 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 4s 686us/step - loss: 4.5253e-04 - acc: 0.9998 - val_loss: 0.0589 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 4s 688us/step - loss: 6.5352e-04 - acc: 0.9998 - val_loss: 0.0582 - val_acc: 0.9880\n",
      "1418/1418 [==============================] - 0s 128us/step\n",
      "\n",
      " accuracy: 0.988\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       626\n",
      "         1.0       0.99      0.99      0.99       792\n",
      "\n",
      "    accuracy                           0.99      1418\n",
      "   macro avg       0.99      0.99      0.99      1418\n",
      "weighted avg       0.99      0.99      0.99      1418\n",
      "\n",
      " 1      1     he 's like , 'yeah i got acne and i love brokeback mountain ' ..\n",
      " 1      1     i liked the first `` mission impossible .\n",
      " 0      0     da vinci code sucked ..\n",
      " 1      1     i love the harry potter series if you can count that as `` a `` book , also catcher in the tye , jane eyre , the virgin suicides , yeah ...\n",
      " 1      1     the da vinci code is awesome ! !\n"
     ]
    }
   ],
   "source": [
    "# 模型構建\n",
    "\n",
    "model = Sequential()\n",
    "# 加『嵌入』層\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "# 加『LSTM』層\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# binary_crossentropy:二分法\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# 模型訓練\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))\n",
    "\n",
    "# 預測\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "predictions = model.predict_classes(Xtest)\n",
    "\n",
    "print(\"\\n accuracy: %.3f\" % (acc))\n",
    "print()\n",
    "print(classification_report(ytest, predictions))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))\n",
    "    \n",
    "\n",
    "model.save(Save_path+\"Mood_LSTM_twitter.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Unmodified oligonucleotides containing the triplex target sequence were purchased from Gibco BRL (Gaithersburg, MD). Target duplexes were formed from a 1:1 mixture of unmodified complementary oligonucleotides in sterile water by denaturing at 80°C for 5 min and slow annealing at room temperature. The duplexes were phosphorylated with T4 polynucleotide kinase (Promega). Inserted sequences were as follows: pCAT-target and pCAT-AatII, AGTTTTGTGTCCC-CCTCTCAGGTGTCACAG; pCAT-Eco72I, AGTTTTGTG-TCCCCCTCTCACGTGTCACAG, where the single base alteration in the sequence (note the bold C) generates an Eco72I site. The pCAT-6target insert (216 bp long) was generated using two oligonucleotides purchased from Genosys (The Woodlands, TX). The first oligonucleotide (108 bp) contained three consecutive target sequences: GCAAGCTTAGTTTTG-TGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCC-TCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGG-TGTCACAGGGATCCGGCG (target sequence AGTTTTGTGTCCCCCTCTCAGGTGTCACAG) and 18 bp of sequence to allow annealing to a second oligonucleotide. The second oligonucleotide (108 bp) also contained three target sequences and 18 bp of sequence complementary to the first oligonucleotide: GCAAGCTTAGTTTTGTGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGGTGTCACAGCGCCGGATC. These two oligonucleotides were mixed in a 1:1 ratio and briefly denatured by heating to 100°C for 5 min and incubated at room temperature for 20 min to allow the complementary 18 bp sequence to anneal. Annealed oligonucleotide was submitted to a DNA polymerase extension procedure using Taq polymerase (Promega). Briefly, 1 pmol of annealed oligonucleotide was incubated at 55°C for 30 min in 50 µl of 50 mM KCl, 10 mM Tris–HCl, pH 9.0, 0.1% Triton X-100, 0.02 mM dATP, dCTP, dGTP and dTTP, 6 mM MgCl2 and 1.25 U Taq DNA polymerase (Promega). After polymerization this mixture was sequentially extracted with an equal volume of phenol:chloroform (1:1) and with an equal volume of chloroform:isoamyl alcohol (24:1) and precipitated with ethanol. Schematics of the plasmids used and target sequences are presented in Figure ​Figure22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified oligonucleotides containing the triplex target sequence were purchased from Gibco BRL (Gaithersburg, MD). Target duplexes were formed from a 1:1 mixture of unmodified complementary oligonucleotides in sterile water by denaturing at 80°C for 5 min and slow annealing at room temperature. The duplexes were phosphorylated with T4 polynucleotide kinase (Promega). Inserted sequences were as follows: pCAT-target and pCAT-AatII, AGTTTTGTGTCCC-CCTCTCAGGTGTCACAG; pCAT-Eco72I, AGTTTTGTG-TCCCCCTCTCACGTGTCACAG, where the single base alteration in the sequence (note the bold C) generates an Eco72I site. The pCAT-6target insert (216 bp long) was generated using two oligonucleotides purchased from Genosys (The Woodlands, TX). The first oligonucleotide (108 bp) contained three consecutive target sequences: GCAAGCTTAGTTTTG-TGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCC-TCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGG-TGTCACAGGGATCCGGCG (target sequence AGTTTTGTGTCCCCCTCTCAGGTGTCACAG) and 18 bp of sequence to allow annealing to a second oligonucleotide. The second oligonucleotide (108 bp) also contained three target sequences and 18 bp of sequence complementary to the first oligonucleotide: GCAAGCTTAGTTTTGTGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGGTGTCACAGCGCCGGATC. These two oligonucleotides were mixed in a 1:1 ratio and briefly denatured by heating to 100°C for 5 min and incubated at room temperature for 20 min to allow the complementary 18 bp sequence to anneal. Annealed oligonucleotide was submitted to a DNA polymerase extension procedure using Taq polymerase (Promega). Briefly, 1 pmol of annealed oligonucleotide was incubated at 55°C for 30 min in 50 µl of 50 mM KCl, 10 mM Tris–HCl, pH 9.0, 0.1% Triton X-100, 0.02 mM dATP, dCTP, dGTP and dTTP, 6 mM MgCl2 and 1.25 U Taq DNA polymerase (Promega). After polymerization this mixture was sequentially extracted with an equal volume of phenol:chloroform (1:1) and with an equal volume of chloroform:isoamyl alcohol (24:1) and precipitated with ethanol. Schematics of the plasmids used and target sequences are presented in Figure ​Figure22.  負面\n"
     ]
    }
   ],
   "source": [
    "mood(\"Input\",Save_path+\"Mood_LSTM_twitter.h5\",data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 10s 2ms/step - loss: 0.2282 - acc: 0.8968 - val_loss: 0.0576 - val_acc: 0.9824\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0287 - acc: 0.9914 - val_loss: 0.0723 - val_acc: 0.9746\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0647 - val_acc: 0.9824\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0495 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0640 - val_acc: 0.9803\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0485 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0521 - val_acc: 0.9866\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0512 - val_acc: 0.9873\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 7.9441e-04 - acc: 0.9998 - val_loss: 0.0595 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0593 - val_acc: 0.9859\n",
      "1418/1418 [==============================] - 0s 224us/step\n",
      "\n",
      " accuracy: 0.986\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       626\n",
      "         1.0       0.98      0.99      0.99       792\n",
      "\n",
      "    accuracy                           0.99      1418\n",
      "   macro avg       0.99      0.99      0.99      1418\n",
      "weighted avg       0.99      0.99      0.99      1418\n",
      "\n",
      " 1      1     i love brokeback mountain .\n",
      " 1      1     i love the da vinci code ...\n",
      " 1      1     i love kirsten / leah / kate escapades and mission impossible tom as well ...\n",
      " 1      1     the da vinci code was absolutely awesome !\n",
      " 0      0     brokeback mountain was boring .\n"
     ]
    }
   ],
   "source": [
    "# 模型構建\n",
    "\n",
    "model = Sequential()\n",
    "# 加『嵌入』層\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "# 加『LSTM』層\n",
    "model.add(Bidirectional(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# binary_crossentropy:二分法\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# 模型訓練\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))\n",
    "\n",
    "# 預測\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "predictions = model.predict_classes(Xtest)\n",
    "\n",
    "print(\"\\n accuracy: %.3f\" % (acc))\n",
    "print()\n",
    "print(classification_report(ytest, predictions))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print(' {}      {}     {}'.format(int(round(ypred)), int(ylabel), sent))\n",
    "    \n",
    "\n",
    "model.save(Save_path+\"Mood_BiLSTM_twitter.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input :  The pCAT-6target insert (216 bp long) was generated using two oligonucleotides purchased from Genosys (The Woodlands, TX). The first oligonucleotide (108 bp) contained three consecutive target sequences: GCAAGCTTAGTTTTG-TGTCCCCCTCTCAGGTGTCACAGAGTTTTGTGTCCCCC-TCTCAGGTGTCACAGAGTTTTGTGTCCCCCTCTCAGG-TGTCACAGGGATCCGGCG (target sequence AGTTTTGTGTCCCCCTCTCAGGTGTCACAG) and 18 bp of sequence to allow annealing to a second oligonucleotide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "負面\n"
     ]
    }
   ],
   "source": [
    "mood(\"Input : \",Save_path+\"Mood_BiLSTM_twitter.h5\",data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
