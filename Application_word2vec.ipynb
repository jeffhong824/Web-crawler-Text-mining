{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from os import listdir,path,rename,walk,makedirs\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pandas import DataFrame\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "# import numpy as np\n",
    "#from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return 'n' #defult pos tag\n",
    "\n",
    "def is_Part_of_speech(tag):\n",
    "    if tag in ['NN', 'NNS', 'NNP', 'NNPS'] :\n",
    "        return 'noun'\n",
    "    \n",
    "    elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] :\n",
    "        return 'verb'\n",
    "    \n",
    "    elif tag in ['RB', 'RBR', 'RBS'] :\n",
    "        return 'adverb'\n",
    "    \n",
    "    elif tag in ['JJ', 'JJR', 'JJS'] :\n",
    "        return 'adjective'\n",
    "    else:\n",
    "        return 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/知識圖譜/Word2vec/'\n",
    "stop_word_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/知識圖譜/Word2vec/'\n",
    "target_path = 'C:/Users/Tingchun.TC.Hung/Desktop/Record/知識圖譜/語料庫/PubMed資料集/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入停用詞\n",
    "stop_word = []\n",
    "with open(stop_word_path+'英文停用詞.txt','r',encoding='utf-8') as file :\n",
    "    for i in file:\n",
    "        stop_word.append(i.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word(txt):\n",
    "    txt = txt.replace('\\ufeff','')\n",
    "    txt = txt.replace('-------------------------------','')\n",
    "    for ch in '!\"#$&()*+,/:;<=>?@[\\\\]^_{|}·~‘’⦁': #-.\n",
    "        txt = txt.replace(ch,\"\")\n",
    "        \n",
    "    stop_sentence = False\n",
    "    if len(txt)!= 0 :\n",
    "        if txt[-1]=='.':\n",
    "            txt = txt.replace('.',\"\")\n",
    "            stop_sentence = True\n",
    "        \n",
    "    return txt,stop_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended=False):\n",
    "\n",
    "        self.__adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        # the first Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "                'last'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?for example (, )?'\n",
    "                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n",
    "                    '(\\\\. )?\\\\))',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "                    'for instance)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "                    '?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        self.__spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        doc = self.__spacy_nlp(rawtext)\n",
    "        chunks = []\n",
    "        for sentence in doc.sents:\n",
    "            sentence_text = sentence.lemma_\n",
    "            for chunk in sentence.noun_chunks:\n",
    "                if chunk.lemma_.lower() == \"example\":\n",
    "                    start = chunk.start\n",
    "                    pre_token = sentence[start - 1].lemma_.lower()\n",
    "                    post_token = sentence[start + 1].lemma_.lower()\n",
    "                    if start > 0 and\\\n",
    "                            (pre_token == \"for\" or post_token == \"of\"):\n",
    "                        continue\n",
    "                if chunk.lemma_.lower() == \"type\":\n",
    "                    continue\n",
    "                chunk_arr = []\n",
    "                replace_arr = []\n",
    "                # print(\"chunk:\", chunk)\n",
    "                for token in chunk:\n",
    "                    if token.lemma_ in self.__adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                        continue\n",
    "                    chunk_arr.append(token.lemma_)\n",
    "                    # Remove punctuation and stopword adjectives\n",
    "                    # (generally quantifiers of plurals)\n",
    "                    if token.lemma_.isalnum():\n",
    "                        replace_arr.append(token.lemma_)\n",
    "                    else:\n",
    "                        replace_arr.append(''.join(\n",
    "                            char for char in token.lemma_ if char.isalnum()\n",
    "                        ))\n",
    "                if len(chunk_arr) == 0:\n",
    "                    chunk_arr.append(chunk[-1].lemma_)\n",
    "                chunk_lemma = ' '.join(chunk_arr)\n",
    "                # print(chunk_lemma)\n",
    "                replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "                if chunk_lemma:\n",
    "                    sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                           r'%s' % replacement_value,\n",
    "                                           sentence_text)\n",
    "            chunks.append(sentence_text)\n",
    "        return chunks\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list\n",
    "        of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged\n",
    "            # into a single NP, it's a chunk error\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "\n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        pair = (\n",
    "                            self.clean_hyponym_term(specifics[i]),\n",
    "                            self.clean_hyponym_term(general)\n",
    "                        )\n",
    "                        # reduce duplicates\n",
    "                        if pair not in hyponyms:\n",
    "                            hyponyms.append(pair)\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HearstPatterns(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_English(each_sentence,Near_word_title,Near_word,near_range):\n",
    "    Near_split = each_sentence.split()\n",
    "    Near_split2 = Near_split[:-1]\n",
    "    new_word = replace_word(Near_split[-1])[0]\n",
    "    if(new_word!=''):\n",
    "        Near_split2.append(new_word)\n",
    "    Near_split = Near_split2  \n",
    "    sentence_long = len(Near_split)\n",
    "    for run_near_word in range(sentence_long):\n",
    "        #若第一次當開頭\n",
    "        if(Near_split[run_near_word] not in Near_word_title):\n",
    "            Near_word_title.append(Near_split[run_near_word])\n",
    "            Near_word.append([[Near_split[run_near_word]],[1]])\n",
    "            if(sentence_long-(run_near_word+1)>(near_range-1)):\n",
    "                for near_words in range(near_range):\n",
    "                    str_near_word = ''\n",
    "                    for add_near_word in range(run_near_word,run_near_word+(near_words+1)+1):\n",
    "                        if(add_near_word!=run_near_word+(near_words+1)):\n",
    "                            str_near_word += Near_split[add_near_word] + ' '\n",
    "                        else:\n",
    "                            str_near_word += Near_split[add_near_word]\n",
    "                    Near_word[-1][0].append(str_near_word)\n",
    "                    Near_word[-1][1].append(1)\n",
    "            else:\n",
    "                str_near_word = Near_split[run_near_word] + ' '\n",
    "                for near_words in range(run_near_word+1,sentence_long):\n",
    "                    if((near_words+1)!=sentence_long):\n",
    "                        str_near_word += Near_split[near_words]\n",
    "                        Near_word[-1][0].append(str_near_word)\n",
    "                        Near_word[-1][1].append(1)\n",
    "                        str_near_word += ' '\n",
    "                    else:\n",
    "                        str_near_word += Near_split[near_words]\n",
    "                        Near_word[-1][0].append(str_near_word)\n",
    "                        Near_word[-1][1].append(1)\n",
    "\n",
    "        #若非第一次當開頭\n",
    "        else :\n",
    "            Near_word_position = Near_word_title.index(Near_split[run_near_word])\n",
    "            Near_word[Near_word_position][1][0] += 1\n",
    "            if(sentence_long-(run_near_word+1)>(near_range-1)):\n",
    "                for near_words in range(near_range):\n",
    "                    str_near_word = ''\n",
    "                    for add_near_word in range(run_near_word,run_near_word+(near_words+1)+1):\n",
    "                        if(add_near_word!=run_near_word+(near_words+1)):\n",
    "                            str_near_word += Near_split[add_near_word] + ' '\n",
    "                        else:\n",
    "                            str_near_word += Near_split[add_near_word]\n",
    "                    if(str_near_word not in Near_word[Near_word_position][0]):\n",
    "                        Near_word[Near_word_position][0].append(str_near_word)\n",
    "                        Near_word[Near_word_position][1].append(1)\n",
    "                    else:\n",
    "                        position_in_apper_list = Near_word[Near_word_position][0].index(str_near_word)\n",
    "                        Near_word[Near_word_position][1][position_in_apper_list] += 1\n",
    "            else:\n",
    "                str_near_word = Near_split[run_near_word] + ' '\n",
    "                for near_words in range(run_near_word+1,sentence_long):\n",
    "                    if((near_words+1)!=sentence_long):\n",
    "                        str_near_word += Near_split[near_words]\n",
    "                        if(str_near_word not in Near_word[Near_word_position][0]):\n",
    "                            Near_word[Near_word_position][0].append(str_near_word)\n",
    "                            Near_word[Near_word_position][1].append(1)\n",
    "                        else:\n",
    "                            position_in_apper_list = Near_word[Near_word_position][0].index(str_near_word)\n",
    "                            Near_word[Near_word_position][1][position_in_apper_list] += 1\n",
    "                        str_near_word += ' '\n",
    "                    else:\n",
    "                        str_near_word += Near_split[near_words]\n",
    "                        if(str_near_word not in Near_word[Near_word_position][0]):\n",
    "                            Near_word[Near_word_position][0].append(str_near_word)\n",
    "                            Near_word[Near_word_position][1].append(1)\n",
    "                        else:\n",
    "                            position_in_apper_list = Near_word[Near_word_position][0].index(str_near_word)\n",
    "                            Near_word[Near_word_position][1][position_in_apper_list] += 1     \n",
    "    return Near_word_title,Near_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attributes=[\n",
    "'is known as',\n",
    "'is composed of',\n",
    "'is a kind of',\n",
    "'is a',\n",
    "'is',\n",
    "'are known as',\n",
    "'are composed of',\n",
    "'are a kind of',\n",
    "'are',\n",
    "'called',\n",
    "'belong'\n",
    "'is belong',\n",
    "'isolated from',\n",
    "'isolates',\n",
    "'was known as',\n",
    "'was composed of',\n",
    "'was a kind of',\n",
    "'was a',\n",
    "'was',\n",
    "'were known as',\n",
    "'were composed of',\n",
    "'were a kind of',\n",
    "'were',\n",
    "'used for'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考文獻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sfhsu29.medium.com/nlp-%E5%B0%88%E6%AC%84-1-2-%E5%A6%82%E4%BD%95%E8%A8%93%E7%B7%B4%E8%87%AA%E5%B7%B1%E7%9A%84-word2vec-5a0754c5cb09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "執行完243篇PubMed論文摘要\n"
     ]
    }
   ],
   "source": [
    "word2vec_trainning_texts = []\n",
    "word2vec_trainning_texts_n = []\n",
    "word2vec_trainning_texts_v = []\n",
    "word2vec_trainning_texts_4type = []\n",
    "\n",
    "word_texts_n = []\n",
    "word_texts_v = []\n",
    "\n",
    "paper_num = 0\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, files in walk(target_path):\n",
    "    for each_paper in files :\n",
    "#         print(each_paper)\n",
    "        with open(target_path+each_paper,'r',encoding='utf-8') as file :\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for each_sentence in sentences :\n",
    "                \n",
    "                pass_sentence = []\n",
    "                pass_sentence_n = []\n",
    "                pass_sentence_v = []\n",
    "                pass_sentence_4type = []\n",
    "                each_sentence_list = each_sentence.split()\n",
    "                \n",
    "                tagged=pos_tag(each_sentence_list)\n",
    "                for each_word, pos in tagged:\n",
    "                    input_Attributes = penn_to_wn(pos)\n",
    "                    Part_of_speech = is_Part_of_speech(pos)\n",
    "                    if(Part_of_speech != 'X'):\n",
    "                        Reduction = lemmatizer.lemmatize(each_word,pos=input_Attributes)\n",
    "                        new_word,end = replace_word(Reduction)\n",
    "                        if(new_word != ''):\n",
    "                            if(new_word.isupper() and len(new_word)>1):\n",
    "                                pass_sentence_4type.append(new_word)\n",
    "                            else:\n",
    "                                if(new_word.lower() not in stop_word ):\n",
    "                                    pass_sentence_4type.append(new_word.lower())\n",
    "                    if(Part_of_speech == 'noun' or Part_of_speech == 'verb'): # if(1):\n",
    "                        Reduction = lemmatizer.lemmatize(each_word,pos=input_Attributes)  \n",
    "                        new_word,end = replace_word(Reduction)\n",
    "                        if(new_word != ''):\n",
    "                            if(new_word.isupper() and len(new_word)>1):\n",
    "                                pass_sentence.append(new_word)\n",
    "                                if(Part_of_speech == 'noun'):\n",
    "                                    pass_sentence_n.append(new_word)\n",
    "                                    if new_word not in word_texts_n:\n",
    "                                        word_texts_n.append(new_word)\n",
    "                                elif(Part_of_speech == 'verb'):\n",
    "                                    pass_sentence_v.append(new_word)\n",
    "                                    if new_word not in word_texts_v:\n",
    "                                        word_texts_v.append(new_word)\n",
    "                            else:\n",
    "                                if(new_word.lower() not in stop_word ):\n",
    "                                    pass_sentence.append(new_word.lower())\n",
    "                                    if(Part_of_speech == 'noun'):\n",
    "                                        pass_sentence_n.append(new_word.lower())\n",
    "                                        if new_word.lower() not in word_texts_n:\n",
    "                                            word_texts_n.append(new_word.lower())\n",
    "                                    elif(Part_of_speech == 'verb'):\n",
    "                                        pass_sentence_v.append(new_word.lower())\n",
    "                                        if new_word.lower() not in word_texts_v:\n",
    "                                            word_texts_v.append(new_word.lower())\n",
    "                if(pass_sentence_4type!=[]):\n",
    "                    word2vec_trainning_texts_4type.append(pass_sentence_4type)\n",
    "                if(pass_sentence!=[]):\n",
    "                    word2vec_trainning_texts.append(pass_sentence)\n",
    "                if(pass_sentence_n!=[]):\n",
    "                    word2vec_trainning_texts_n.append(pass_sentence_n)\n",
    "                if(pass_sentence_v!=[]):\n",
    "                    word2vec_trainning_texts_v.append(pass_sentence_v)\n",
    "            paper_num += 1\n",
    "print('執行完'+str(paper_num)+'篇PubMed論文摘要')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含4種type句子 :  2474\n",
      "包含名詞動詞句子 :  2470\n",
      "名詞句子 :  2466\n",
      "動詞句子 :  1963\n"
     ]
    }
   ],
   "source": [
    "print('包含4種type句子 : ',len(word2vec_trainning_texts_4type))\n",
    "print('包含名詞動詞句子 : ',len(word2vec_trainning_texts))\n",
    "print('名詞句子 : ',len(word2vec_trainning_texts_n))\n",
    "print('動詞句子 : ',len(word2vec_trainning_texts_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "看到名詞(不重複)出現字數 :  3858\n",
      "看到動詞(不重複)出現字數 :  935\n"
     ]
    }
   ],
   "source": [
    "print('看到名詞(不重複)出現字數 : ',len(word_texts_n))\n",
    "print('看到動詞(不重複)出現字數 : ',len(word_texts_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=word2vec_trainning_texts, window=5, min_count=1, workers=4) #, vector_size=100\n",
    "model.save(save_path+\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提出訓練好的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(save_path+\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一個字用幾維vector表示 : 100\n"
     ]
    }
   ],
   "source": [
    "print('一個字用幾維vector表示 : '+str(model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 字數 : 4422\n",
      "實例 : \n",
      "removal\n",
      "infectivity\n",
      "blood\n",
      "product\n",
      "prepare\n"
     ]
    }
   ],
   "source": [
    "vocab_list = list(model.wv.vocab.keys())\n",
    "print('word2vec 字數 : '+str(len(vocab_list)))\n",
    "print('實例 : ')\n",
    "for i in vocab_list[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RNA', 0.9881764650344849)\n",
      "('gene', 0.987464964389801)\n",
      "('infection', 0.9867336750030518)\n",
      "('PCR', 0.9846532940864563)\n",
      "('particle', 0.9832759499549866)\n",
      "('protein', 0.9822303652763367)\n",
      "('sequence', 0.9820447564125061)\n",
      "('detection', 0.9819107055664062)\n",
      "('assay', 0.9817799925804138)\n",
      "('cell', 0.9816744923591614)\n"
     ]
    }
   ],
   "source": [
    "ans = model.wv.most_similar(positive=['virus'], negative=[], topn=10)\n",
    "# ans = model.wv.most_similar(positive=['virus','PCR'], negative=[], topn=10)\n",
    "# ans = model.wv.most_similar(positive=['virus'], negative=['PCR'], topn=10)\n",
    "for i in ans :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus & PCR距離 : \n",
      "0.01534658670425415\n"
     ]
    }
   ],
   "source": [
    "word_a = 'virus'\n",
    "word_b = 'PCR'\n",
    "\n",
    "two_word_distance = model.wv.distance(word_a,word_b)\n",
    "print(word_a+' & '+word_b+'距離 : ')\n",
    "print(two_word_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05425971 -0.03182517 -0.0096318  -0.01050142 -0.00216749 -0.06463587\n",
      " -0.01695463 -0.02169042  0.02870228 -0.01211303 -0.00588886  0.02702897\n",
      " -0.05834166 -0.05605504 -0.07637904  0.05305203  0.04512161  0.00370761\n",
      "  0.03353662  0.00847748  0.00713624 -0.00921729 -0.00103337  0.03733119\n",
      " -0.00148212  0.0012035   0.01043321  0.04415888 -0.0044337  -0.01072861\n",
      "  0.05931643  0.01644801 -0.03724993  0.02106954  0.03184151  0.05113804\n",
      "  0.01292499  0.03384535  0.00741403 -0.01483723 -0.04139743  0.07878213\n",
      "  0.03536803  0.00037794  0.04338795 -0.03010762  0.013257   -0.02036365\n",
      " -0.03146609  0.01029766  0.01629391 -0.00337227  0.033557    0.03696047\n",
      " -0.03500364  0.02248935  0.04040173 -0.00290783 -0.00404452  0.04770268\n",
      "  0.04094021  0.04918053 -0.01345428  0.01729371  0.0282013   0.00074109\n",
      "  0.03087345 -0.00090332  0.01267883 -0.09072983  0.0052141  -0.01190906\n",
      "  0.00413738 -0.03697195 -0.02612349 -0.03975829  0.05680097  0.01543936\n",
      "  0.03902845 -0.02407588  0.00382254  0.01514424 -0.00240664  0.00686661\n",
      "  0.00973632  0.05583742  0.01410414 -0.03575938  0.04388457 -0.01792318\n",
      "  0.0207948  -0.0322367  -0.04946887 -0.0047443   0.0341023   0.00725315\n",
      " -0.01968226 -0.02645432  0.08484463 -0.03797227]\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv.get_vector('virus')\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('virus', 1.0),\n",
       " ('RNA', 0.9881764650344849),\n",
       " ('gene', 0.987464964389801),\n",
       " ('infection', 0.9867337346076965),\n",
       " ('PCR', 0.9846532940864563),\n",
       " ('particle', 0.9832760095596313),\n",
       " ('protein', 0.9822304248809814),\n",
       " ('sequence', 0.9820447564125061),\n",
       " ('detection', 0.9819107055664062),\n",
       " ('assay', 0.9817799925804138)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑選成NE字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_texts_n_NE = word_texts_n[:1000]\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_n_sort_dis = []\n",
    "index_name = []\n",
    "for each_n in word_texts_n_NE :\n",
    "    sort_word = model.wv.most_similar(positive=[each_n], negative=[], topn=len(word_texts_n)+len(word_texts_v))\n",
    "    list_n_sort_dis_this = []\n",
    "    for each_distance in sort_word :\n",
    "        if(each_distance[1]>=threshold):\n",
    "            if each_distance[0] in word_texts_n_NE:\n",
    "                list_n_sort_dis_this.append(each_distance)\n",
    "    list_n_sort_dis.append(list_n_sort_dis_this)\n",
    "    index_name.append(each_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary_word2vec = DataFrame(list_n_sort_dis,index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           (virus, 0.9779177308082581)\n",
       "1             (RNA, 0.9754770398139954)\n",
       "2        (sequence, 0.9753614664077759)\n",
       "3       (infection, 0.9752068519592285)\n",
       "4            (gene, 0.9737394452095032)\n",
       "                     ...               \n",
       "319       (monodon, 0.5008969306945801)\n",
       "320    (difficulty, 0.5002762079238892)\n",
       "321                                None\n",
       "322                                None\n",
       "323                                None\n",
       "Name: DNA, Length: 324, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionary_word2vec.loc[\"DNA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用於找領域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus & PCR距離 : \n",
      "0.01534658670425415\n"
     ]
    }
   ],
   "source": [
    "word_a = 'virus'\n",
    "word_b = 'PCR'\n",
    "\n",
    "two_word_distance = model.wv.distance(word_a,word_b)\n",
    "print(word_a+' & '+word_b+'距離 : ')\n",
    "print(two_word_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus & acid距離 : \n",
      "0.045733869075775146\n"
     ]
    }
   ],
   "source": [
    "word_a = 'virus'\n",
    "word_b = 'acid'\n",
    "\n",
    "two_word_distance = model.wv.distance(word_a,word_b)\n",
    "print(word_a+' & '+word_b+'距離 : ')\n",
    "print(two_word_distance)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nucleic acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HearstPatterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mmichelsonIF/hearst_patterns_python/tree/master/hearstPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HearstPatterns(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('postharvest loss reduction', 'benefit'),\n",
       " ('food increase', 'benefit'),\n",
       " ('soil fertility improvement', 'benefit'),\n",
       " ('benefit', 'postharvest loss reduction')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\"\n",
    "h.find_hyponyms(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料庫統計成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "執行完243篇PubMed論文摘要\n"
     ]
    }
   ],
   "source": [
    "paper_num = 0\n",
    "\n",
    "Dictionary_HearstPatterns_word = []\n",
    "Dictionary_HearstPatterns_time = []\n",
    "\n",
    "for root, dirs, files in walk(target_path):\n",
    "    for each_paper in files :\n",
    "#         print(each_paper)\n",
    "        with open(target_path+each_paper,'r',encoding='utf-8') as file :\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for each_sentence in sentences :\n",
    "                Dictionary_HearstPatterns = h.find_hyponyms(each_sentence)\n",
    "                for each_same in Dictionary_HearstPatterns :\n",
    "                    if each_same in Dictionary_HearstPatterns_word :\n",
    "                        position = Dictionary_HearstPatterns_word.index(each_same)\n",
    "                        Dictionary_HearstPatterns_time[position] += 1\n",
    "                    else:\n",
    "                        Dictionary_HearstPatterns_word.append(each_same)\n",
    "                        Dictionary_HearstPatterns_time.append(1)\n",
    "            paper_num += 1\n",
    "print('執行完'+str(paper_num)+'篇PubMed論文摘要')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_H_set = []\n",
    "D_H_set_w = []\n",
    "for First_w in range(len(Dictionary_HearstPatterns_word)) :\n",
    "    if Dictionary_HearstPatterns_word[First_w][0] not in D_H_set_w :\n",
    "        D_H_set.append([[Dictionary_HearstPatterns_word[First_w][1]],[Dictionary_HearstPatterns_time[First_w]]])\n",
    "        D_H_set_w.append(Dictionary_HearstPatterns_word[First_w][0])\n",
    "    else:\n",
    "        position = D_H_set_w.index(Dictionary_HearstPatterns_word[First_w][0])\n",
    "        D_H_set[position][0].append(Dictionary_HearstPatterns_word[First_w][1])\n",
    "        D_H_set[position][1].append(Dictionary_HearstPatterns_time[First_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionart_HearstPatterns = DataFrame(D_H_set,index=D_H_set_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the cell growth  promote factor</th>\n",
       "      <td>[protein]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RIV</th>\n",
       "      <td>[a putative HIV vaccine]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CS mutation</th>\n",
       "      <td>[the clinical isolate]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a high sensitivity</th>\n",
       "      <td>[the EM analysis]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some virus</th>\n",
       "      <td>[microbe]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the Enhanced Green Fluorescent Protein</th>\n",
       "      <td>[a marker]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chromosomal analysis</th>\n",
       "      <td>[further study]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug treatment complication</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radiation effect</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recurrent tumor</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0    1\n",
       "the cell growth  promote factor                        [protein]  [2]\n",
       "RIV                                     [a putative HIV vaccine]  [1]\n",
       "CS mutation                               [the clinical isolate]  [1]\n",
       "a high sensitivity                             [the EM analysis]  [1]\n",
       "some virus                                             [microbe]  [1]\n",
       "the Enhanced Green Fluorescent Protein                [a marker]  [1]\n",
       "chromosomal analysis                             [further study]  [1]\n",
       "drug treatment complication               [noninfectious entity]  [1]\n",
       "radiation effect                          [noninfectious entity]  [1]\n",
       "recurrent tumor                           [noninfectious entity]  [1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionart_HearstPatterns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [microbe]\n",
       "1          [1]\n",
       "Name: some virus, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionart_HearstPatterns.loc[\"some virus\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "such as 的關聯無法找出縮寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIV\n",
      "0    [a putative HIV vaccine]\n",
      "1                         [1]\n",
      "Name: RIV, dtype: object\n",
      "\n",
      "\n",
      "PCR\n",
      "0    [systematic laboratory testing]\n",
      "1                                [1]\n",
      "Name: PCR, dtype: object\n",
      "\n",
      "\n",
      "CSF\n",
      "0    [sample, a false  positive result]\n",
      "1                                [1, 1]\n",
      "Name: CSF, dtype: object\n",
      "\n",
      "\n",
      "HIV1\n",
      "0    [sexually transmit disease]\n",
      "1                            [1]\n",
      "Name: HIV1, dtype: object\n",
      "\n",
      "\n",
      "EHV2\n",
      "0    [etiological agent]\n",
      "1                    [1]\n",
      "Name: EHV2, dtype: object\n",
      "\n",
      "\n",
      "EHV5\n",
      "0    [etiological agent]\n",
      "1                    [1]\n",
      "Name: EHV5, dtype: object\n",
      "\n",
      "\n",
      "PCV2\n",
      "0    [10 swine virus]\n",
      "1                 [1]\n",
      "Name: PCV2, dtype: object\n",
      "\n",
      "\n",
      "PRRSV\n",
      "0    [10 swine virus, pathogen transmission]\n",
      "1                                     [1, 1]\n",
      "Name: PRRSV, dtype: object\n",
      "\n",
      "\n",
      "CHIKV\n",
      "0    [positive result]\n",
      "1                  [1]\n",
      "Name: CHIKV, dtype: object\n",
      "\n",
      "\n",
      "ELISA\n",
      "0    [serological test]\n",
      "1                   [1]\n",
      "Name: ELISA, dtype: object\n",
      "\n",
      "\n",
      "HIV1 RNA\n",
      "0    [a model]\n",
      "1          [1]\n",
      "Name: HIV1 RNA, dtype: object\n",
      "\n",
      "\n",
      "TS\n",
      "0    [sample]\n",
      "1         [1]\n",
      "Name: TS, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index_ in Dictionart_HearstPatterns.index :\n",
    "    if(index_.isupper() and len(index_)>1):\n",
    "        print(index_)\n",
    "        print(Dictionart_HearstPatterns.loc[index_])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文斷詞統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'have', 'a', 'fat', 'cat'],\n",
       " [[['I', 'I have', 'I have a'], [1, 1, 1]],\n",
       "  [['have', 'have a', 'have a fat'], [1, 1, 1]],\n",
       "  [['a', 'a fat', 'a fat cat'], [1, 1, 1]],\n",
       "  [['fat', 'fat cat'], [1, 1]],\n",
       "  [['cat'], [1]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Near_word_title = []\n",
    "Near_word = []\n",
    "near_range = 2\n",
    "\n",
    "EX_Sentence = 'I have a fat cat.'\n",
    "tokenize_English(EX_Sentence,Near_word_title,Near_word,near_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 屬性字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](C:/Users/Tingchun.TC.Hung/Desktop/Record/知識圖譜/關係預定詞.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus is unimpaired\n",
      "\n",
      "virus was removed by the sCV-N, leaving behind a relatively larger fraction of non-infectious virus in the supernatant which we designated as replication incompetent virions (RIV)\n",
      "\n",
      "virus is non-infectious in transfected cell culture and in injected sheep\n",
      "\n",
      "virus was exchanged with the prt region from the Belgian provirus\n",
      "\n",
      "virus were obtained, JV persistently infected cells became morphologically undistinguishable from Vero cells and virus production dropped to undetectable levels\n",
      "\n",
      "virus was purified by plaque assay on Sf9 cells\n",
      "\n",
      "virus are common in infancy, causing mostly asymptomatic infections\n",
      "\n",
      "virus was associated with fever and with symptoms of cold but not with diarrhoea and vomiting\n",
      "\n",
      "virus was enhanced notably at that moment\n",
      "\n",
      "virus was performed, and the resulting products were spliced into a fragment which was packaged into armored RNA for use as a noninfectious, quantifiable synthetic substitute\n",
      "\n",
      "virus isolated from China (TMV-Cv)\n",
      "\n",
      "virus was detected in lymphoid tissue homogenates of six of 13 pigs by intramuscular bioassay\n",
      "\n",
      "virus isolates were identified by neutralization, immunofluorescence assay, or enzyme immunoassay\n",
      "\n",
      "virus were rendered noninfectious or inactivated by treatment with heat (72 degrees C, 37 degrees C, and 19 degrees C) or hypochlorite\n",
      "\n",
      "virus are important to curtail the spread of this virus\n",
      "\n",
      "virus was identified as the most frequent cause of infectious uveitis in both HIV-negative and HIV-positive patients (49 and 91%, respectively)\n",
      "\n",
      "virus is the most important foodborne virus in Japan\n",
      "\n",
      "virus were similar\n",
      "\n",
      "virus were detected from AcBAC109KO DNA-transfected cells compared to the parental virus using Q-PCR to detect viral DNA or by immunoblotting to detect a budded virus protein\n",
      "\n",
      "virus are increasingly recognized worldwide as the most important cause of food borne gastro-intestinal illness\n",
      "\n",
      "virus is probably the main causative agent of Fuchs uveitis, but other viruses may also be involved in the pathogenesis of this disease\n",
      "\n",
      "virus is commonly found in environmental waters and is very resistant to water disinfection and environmental stressors, especially UV light inactivation\n",
      "\n",
      "virus was evaluated with three RT-qPCR assays (A, B, C) during thermal inactivation kinetics (at 37°C, 68 C, 72°C, 80°C) through comparison with data obtained by RT-qPCR and by infectious titration in cell culture\n",
      "\n",
      "virus is regarded as the major risk factor in the development of cervical cancer\n",
      "\n",
      "virus is mainly maintained through human-mosquito-human cycle\n",
      "\n",
      "virus were inoculated to well-grown monolayer cells and then calculated the concentration of virus through the cytopathic effect (CPE)\n",
      "\n",
      "virus were prepared in a fixed concentration of 106 PFU/mL then diluted in ten-time gradient and the DNA of virus were extracted and PCR were applied, then the target DNA fragments were observed through gel electrophoresis\n",
      "\n",
      "virus were treated with different concentrations of EMA (0 μg/mL, 70 μg/mL, 120 μg/mL and 150 μg/mL) and then detected the target DNA fragment in the same way\n",
      "\n",
      "virus were suppressed by EMA at 120 μg/mL and 150 μg/mL concentration, while the DNA amplification of infectious adenovirus in concentration of 105 PFU/mL and above were not affected by 120 μg/mL EMA\n",
      "\n",
      "virus were treated with PMA/PMAxx, benzonase or crude extract RNase prior to RT-qPCR\n",
      "\n",
      "virus is a globally spread zoonotic arbovirus\n",
      "\n",
      "virus was inhibited in a dose- and genotype-dependent manner by direct-acting-antivirals targeting NS3/4A, NS5A, and NS5B\n",
      "\n",
      "virus was rhinovirus in nasal swab or sputum, compared to Haemophilus parainfluenzae in sputum and influenza A in nasal swab or sputum in mild bronchiectasis\n",
      "\n",
      "virus is recognised as a significant food safety risk\n",
      "\n",
      "virus was found in both sets of samples, with more samples positive for genogroup II phage, at generally higher levels than norovirus\n",
      "\n",
      "virus was no longer detectable was 31\n",
      "\n",
      "virus is primarily transmitted from person to person through inhalation of infected droplets or aerosols\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Search_Attributes_target = 'virus'\n",
    "\n",
    "a = 0\n",
    "\n",
    "for root, dirs, files in walk(target_path):\n",
    "    for each_paper in files :\n",
    "\n",
    "        with open(target_path+each_paper,'r',encoding='utf-8') as file :\n",
    "            each_prepose_paper = file.read()\n",
    "\n",
    "            a+=1\n",
    "            text = each_prepose_paper\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for each_sentence in sentences :\n",
    "                for Attributes_situation in Attributes :\n",
    "                    target_position = each_sentence.find(Search_Attributes_target+' '+Attributes_situation+' ')\n",
    "                    if(target_position!=-1):\n",
    "\n",
    "                        Attributes_Sentence = each_sentence[target_position:target_position+each_sentence[target_position:].find('.')]\n",
    "                        Attributes_Sentence2 = Attributes_Sentence.split()\n",
    "                        Attributes_Sentence_target =  len(Attributes_Sentence2)-len(each_sentence[len(Search_Attributes_target+' '+Attributes_situation)+target_position+1 :].split())\n",
    "\n",
    "                        tagged = pos_tag(Attributes_Sentence2)\n",
    "                        case = []\n",
    "                        for each_word, pos in tagged[Attributes_Sentence_target:] :\n",
    "\n",
    "                            Part_of_speech = is_Part_of_speech(pos)\n",
    "                            case.append(Part_of_speech)\n",
    "        #                     print(each_word,Part_of_speech)\n",
    "                        if('adjective' in case or 'noun' in case):\n",
    "                            print(Attributes_Sentence)\n",
    "                            print()\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看第一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "執行完243篇PubMed論文摘要\n"
     ]
    }
   ],
   "source": [
    "word2vec_trainning_texts = []\n",
    "word2vec_trainning_texts_n = []\n",
    "word2vec_trainning_texts_v = []\n",
    "word2vec_trainning_texts_4type = []\n",
    "\n",
    "word_texts_n = []\n",
    "word_texts_v = []\n",
    "\n",
    "paper_num = 0\n",
    "\n",
    "raw_papers = []\n",
    "new_papers = []\n",
    "\n",
    "Dictionary_HearstPatterns_word = []\n",
    "Dictionary_HearstPatterns_time = []\n",
    "\n",
    "Dictionary_abbreviation = []\n",
    "\n",
    "Near_word_title = []\n",
    "Near_word = []\n",
    "near_range = 2\n",
    "\n",
    "not_abbreviation = ['I','II','III','IV','V','VI','VII','VIII','VV']\n",
    "\n",
    "for root, dirs, files in walk(target_path):\n",
    "    for each_paper in files :\n",
    "#         print(each_paper)\n",
    "        paper_now = ''\n",
    "        with open(target_path+each_paper,'r',encoding='utf-8') as file :\n",
    "            text = file.read()\n",
    "            raw_papers.append(text)\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for each_sentence in sentences :\n",
    "                \n",
    "                #####\n",
    "                #統計斷詞頻率\n",
    "                Near_word_title,Near_word = tokenize_English(each_sentence,Near_word_title,Near_word,near_range)      \n",
    "                #####\n",
    "                \n",
    "                #####\n",
    "                #尋找符合HearstPatterns之關係\n",
    "                Dictionary_HearstPatterns = h.find_hyponyms(each_sentence)\n",
    "                for each_same in Dictionary_HearstPatterns :\n",
    "                    if each_same in Dictionary_HearstPatterns_word :\n",
    "                        position = Dictionary_HearstPatterns_word.index(each_same)\n",
    "                        Dictionary_HearstPatterns_time[position] += 1\n",
    "                    else:\n",
    "                        Dictionary_HearstPatterns_word.append(each_same)\n",
    "                        Dictionary_HearstPatterns_time.append(1)      \n",
    "                #####\n",
    "                \n",
    "                pass_sentence = []\n",
    "                pass_sentence_n = []\n",
    "                pass_sentence_v = []\n",
    "                pass_sentence_4type = []\n",
    "                each_sentence_list = each_sentence.split()\n",
    "                \n",
    "                # POS Tagging -(1)\n",
    "                tagged = pos_tag(each_sentence_list)\n",
    "                for each_word, pos in tagged:\n",
    "                    \n",
    "                    #####\n",
    "                    # POS Tagging -(2)簡單化 4 Type \n",
    "                    Part_of_speech = is_Part_of_speech(pos)\n",
    "                    #####\n",
    "                    \n",
    "                    # 詞形還原 -(1)\n",
    "                    input_Attributes = penn_to_wn(pos)\n",
    "                    \n",
    "                    #####\n",
    "                    # 過濾成新的 Papers\n",
    "                    # 收集word2vec用的 4 Type 庫\n",
    "                    if(Part_of_speech != 'X'): #唯有 4 Type才做\n",
    "                        # 詞形還原 -(2)還原\n",
    "                        Reduction = lemmatizer.lemmatize(each_word,pos=input_Attributes)\n",
    "                        new_word,end = replace_word(Reduction)\n",
    "                        if(new_word != ''):\n",
    "                            if(new_word.isupper() and len(new_word)>1):\n",
    "                                pass_sentence_4type.append(new_word)\n",
    "                                paper_now += new_word +' '\n",
    "                            else:\n",
    "                                if(new_word.lower() not in stop_word ):\n",
    "                                    pass_sentence_4type.append(new_word.lower())\n",
    "                                    paper_now += new_word.lower() +' '\n",
    "                            if(end):\n",
    "                                paper_now += '. '\n",
    "                    #####\n",
    "                    \n",
    "                    #####\n",
    "                    # 收集word2vec用的 名詞庫、動詞庫、名動詞庫\n",
    "                    if(Part_of_speech == 'noun' or Part_of_speech == 'verb'): #唯有 N / V才做 # if(1):\n",
    "                        Reduction = lemmatizer.lemmatize(each_word,pos=input_Attributes)  \n",
    "                        new_word,end = replace_word(Reduction)\n",
    "                        if(new_word != ''):\n",
    "                            # ~~~如果是縮寫~~~\n",
    "                            if(new_word.isupper() and len(new_word)>1 and new_word not in not_abbreviation):\n",
    "                                pass_sentence.append(new_word)\n",
    "                                if(new_word not in Dictionary_abbreviation) :\n",
    "                                    Dictionary_abbreviation.append(new_word)\n",
    "                                if(Part_of_speech == 'noun'):\n",
    "                                    pass_sentence_n.append(new_word)\n",
    "                                    if new_word not in word_texts_n:\n",
    "                                        word_texts_n.append(new_word)\n",
    "                                elif(Part_of_speech == 'verb'):\n",
    "                                    pass_sentence_v.append(new_word)\n",
    "                                    if new_word not in word_texts_v:\n",
    "                                        word_texts_v.append(new_word)\n",
    "                                ###\n",
    "                                \n",
    "                                ###\n",
    "                            else:\n",
    "                                if(new_word.lower() not in stop_word ):\n",
    "                                    pass_sentence.append(new_word.lower())\n",
    "                                    if(Part_of_speech == 'noun'):\n",
    "                                        pass_sentence_n.append(new_word.lower())\n",
    "                                        if new_word.lower() not in word_texts_n:\n",
    "                                            word_texts_n.append(new_word.lower())\n",
    "                                    elif(Part_of_speech == 'verb'):\n",
    "                                        pass_sentence_v.append(new_word.lower())\n",
    "                                        if new_word.lower() not in word_texts_v:\n",
    "                                            word_texts_v.append(new_word.lower())\n",
    "                    #####\n",
    "                    \n",
    "                ##### 收集word2vec用\n",
    "                if(pass_sentence_4type!=[]):\n",
    "                    word2vec_trainning_texts_4type.append(pass_sentence_4type)\n",
    "                if(pass_sentence!=[]):\n",
    "                    word2vec_trainning_texts.append(pass_sentence)\n",
    "                if(pass_sentence_n!=[]):\n",
    "                    word2vec_trainning_texts_n.append(pass_sentence_n)\n",
    "                if(pass_sentence_v!=[]):\n",
    "                    word2vec_trainning_texts_v.append(pass_sentence_v)\n",
    "                    \n",
    "            ##### 過濾成新的 Papers\n",
    "            new_papers.append(paper_now) \n",
    "            \n",
    "            paper_num += 1\n",
    "print('執行完'+str(paper_num)+'篇PubMed論文摘要')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 斷詞字典"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "建立字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Near_word_final = []\n",
    "for sorting in range(len(Near_word)):\n",
    "    list1 , list2 = (list(t) for t in zip(*sorted(zip(Near_word[sorting][1],Near_word[sorting][0]))))\n",
    "    list1 = list1[::-1]\n",
    "    list2 = list2[::-1]\n",
    "    Near_word_final.append([list2,list1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "字典搜尋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propidium monoazide (PMA) 5  True \n",
      "propidium monoazide 5  True \n",
      "propidium 5  True \n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "ID = Near_word_title.index('propidium') #polymerase #deoxyribonucleic #propidium monoazide\n",
    "threshold_tokenize = 0.5\n",
    "for showwing in range(5):\n",
    "    try:\n",
    "        tokenize_yes_no_number = Near_word_final[ID][1][showwing]/Near_word[ID][1][0] #用原始單獨自己來算\n",
    "        if(tokenize_yes_no_number>=threshold_tokenize):\n",
    "            print(Near_word_final[ID][0][showwing],Near_word_final[ID][1][showwing],' True ')\n",
    "        else:\n",
    "            print(Near_word_final[ID][0][showwing],Near_word_final[ID][1][showwing],' False ')\n",
    "    except:\n",
    "        print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含4種type句子 :  2474\n",
      "包含名詞動詞句子 :  2470\n",
      "名詞句子 :  2466\n",
      "動詞句子 :  1963\n"
     ]
    }
   ],
   "source": [
    "print('包含4種type句子 : ',len(word2vec_trainning_texts_4type))\n",
    "print('包含名詞動詞句子 : ',len(word2vec_trainning_texts))\n",
    "print('名詞句子 : ',len(word2vec_trainning_texts_n))\n",
    "print('動詞句子 : ',len(word2vec_trainning_texts_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "看到名詞(不重複)出現字數 :  3856\n",
      "看到動詞(不重複)出現字數 :  935\n"
     ]
    }
   ],
   "source": [
    "print('看到名詞(不重複)出現字數 : ',len(word_texts_n))\n",
    "print('看到動詞(不重複)出現字數 : ',len(word_texts_v))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "使用 4種詞性句子 or 包含動詞名詞句子 訓練word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=word2vec_trainning_texts, window=5, min_count=1, workers=4) #, vector_size=100\n",
    "model.save(save_path+\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提出訓練好的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(save_path+\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一個字用幾維vector表示 : 100\n"
     ]
    }
   ],
   "source": [
    "print('一個字用幾維vector表示 : '+str(model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 字數 : 4420\n",
      "實例 : \n",
      "removal\n",
      "infectivity\n",
      "blood\n",
      "product\n",
      "prepare\n"
     ]
    }
   ],
   "source": [
    "vocab_list = list(model.wv.vocab.keys())\n",
    "print('word2vec 字數 : '+str(len(vocab_list)))\n",
    "print('實例 : ')\n",
    "for i in vocab_list[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('infection', 0.9886000752449036)\n",
      "('RNA', 0.9872105121612549)\n",
      "('sequence', 0.9859580397605896)\n",
      "('DNA', 0.9849257469177246)\n",
      "('gene', 0.9842233657836914)\n",
      "('assay', 0.9836938381195068)\n",
      "('PCR', 0.983464777469635)\n",
      "('cell', 0.983388364315033)\n",
      "('infect', 0.9817866086959839)\n",
      "('detection', 0.9798007607460022)\n"
     ]
    }
   ],
   "source": [
    "ans = model.wv.most_similar(positive=['virus'], negative=[], topn=10)\n",
    "# ans = model.wv.most_similar(positive=['virus','PCR'], negative=[], topn=10)\n",
    "# ans = model.wv.most_similar(positive=['virus'], negative=['PCR'], topn=10)\n",
    "for i in ans :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus & PCR距離 : \n",
      "0.016535162925720215\n"
     ]
    }
   ],
   "source": [
    "word_a = 'virus'\n",
    "word_b = 'PCR'\n",
    "\n",
    "two_word_distance = model.wv.distance(word_a,word_b)\n",
    "print(word_a+' & '+word_b+'距離 : ')\n",
    "print(two_word_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05876313 -0.05356771  0.03455362  0.05839892 -0.10250296  0.00908248\n",
      " -0.04107442 -0.00453115  0.0524398  -0.04141151 -0.06854161 -0.00735937\n",
      " -0.00759399  0.05934818  0.00347914  0.04573302  0.00324655  0.01283675\n",
      " -0.02702908  0.02888013 -0.03884978 -0.01668879  0.01754783 -0.03461919\n",
      "  0.00933287  0.01970799  0.00562799  0.0256945  -0.06604835  0.01200862\n",
      "  0.00087849  0.00362029 -0.04118326  0.0092281  -0.02482081 -0.00998114\n",
      "  0.02297064  0.03125003 -0.03126125 -0.00669038 -0.04350164  0.0069631\n",
      " -0.02895527 -0.06841522  0.01686334  0.01327391  0.04455603  0.00570064\n",
      "  0.00470009  0.02595395  0.00952964 -0.0266767   0.04134976 -0.02948535\n",
      " -0.01852163 -0.00595095 -0.03237408  0.06278611  0.01658367  0.03790691\n",
      " -0.02345408  0.03374553 -0.02021487 -0.07476577  0.00677003  0.02588035\n",
      " -0.0045572  -0.02993117  0.04127441  0.00429658 -0.04622592 -0.00047428\n",
      "  0.04528968 -0.02828533  0.04694491  0.00957428 -0.06808792 -0.00245372\n",
      "  0.01973607  0.03016105  0.02067111  0.01927081  0.00011183 -0.01264452\n",
      " -0.00743856 -0.01485243  0.08156342 -0.01582862  0.02321703 -0.00234923\n",
      "  0.02126322  0.00246068  0.02865412  0.01288859 -0.01296781  0.04070674\n",
      " -0.03305543 -0.02675424 -0.0052726  -0.01888104]\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv.get_vector('virus')\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('virus', 1.0),\n",
       " ('infection', 0.9886000752449036),\n",
       " ('RNA', 0.9872105121612549),\n",
       " ('sequence', 0.9859580397605896),\n",
       " ('DNA', 0.9849257469177246),\n",
       " ('gene', 0.9842233657836914),\n",
       " ('assay', 0.9836938381195068),\n",
       " ('PCR', 0.9834648370742798),\n",
       " ('cell', 0.9833883047103882),\n",
       " ('infect', 0.9817866683006287)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用於找領域"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is the full name of HERV-K in the PCR field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: use options instead of chrome_options\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "chrome_options=Options()\n",
    "chrome_options.add_argument('--headless')#無介面操作\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "def google_search(word_a,Domain,open_or_not = False):\n",
    "    try:\n",
    "        driver.get(\"https://www.google.com/search?q=what is the full name of \"+word_a+\" in \"+Domain+\" field\")\n",
    "        Abstract_wiki = driver.find_element_by_xpath('//*[@id=\"rso\"]/div[1]/div[1]/div/div[1]/div').text\n",
    "        if(open_or_not):\n",
    "            print(Abstract_wiki)\n",
    "            print('-----------')\n",
    "        if(Abstract_wiki[:6] != '翻譯這個網頁'):\n",
    "            chinese = Abstract_wiki.find('網路上的精選摘要')\n",
    "            chinese2 = Abstract_wiki.find('全部顯示')\n",
    "            if(chinese!=-1 and chinese2!=-1):\n",
    "                if(open_or_not):\n",
    "                    print('網路上的精選摘要&全部顯示')\n",
    "                if(Abstract_wiki.find(' ('+word_a+')')!=-1):\n",
    "                    sentence_abbreviation = Abstract_wiki[14:Abstract_wiki.find(' ('+word_a+')')]\n",
    "                else:\n",
    "                    if(Abstract_wiki.find(word_a+' (')!=-1):\n",
    "                        start_position = Abstract_wiki.find(word_a+' (')+len(word_a)+2\n",
    "                        end_position = Abstract_wiki[start_position:].find(')')\n",
    "                        sentence_abbreviation = Abstract_wiki[start_position:start_position+end_position]\n",
    "                        if(open_or_not):\n",
    "                            print(sentence_abbreviation)\n",
    "                    else:\n",
    "                        sentence_abbreviation = Abstract_wiki[14:Abstract_wiki.find(' ('+word_a+')')]\n",
    "            elif(chinese!=-1):\n",
    "                if(open_or_not):\n",
    "                    print('網路上的精選摘要')\n",
    "                if(Abstract_wiki.find(' ('+word_a+')')!=-1):\n",
    "                    sentence_abbreviation = Abstract_wiki[9:Abstract_wiki.find(' ('+word_a+')')]\n",
    "                else:\n",
    "                    if(Abstract_wiki.find(word_a+' (')!=-1):\n",
    "                        start_position = Abstract_wiki.find(word_a+' (')+len(word_a)+2\n",
    "                        end_position = Abstract_wiki[start_position:].find(')')\n",
    "                        sentence_abbreviation = Abstract_wiki[start_position:start_position+end_position]\n",
    "                        if(open_or_not):\n",
    "                            print(sentence_abbreviation)\n",
    "                    else:\n",
    "                        sentence_abbreviation = Abstract_wiki[9:Abstract_wiki.find(' ('+word_a+')')]\n",
    "            elif(chinese2!=-1):\n",
    "                if(open_or_not):\n",
    "                    print('全部顯示')\n",
    "                if(Abstract_wiki.find(' ('+word_a+')')!=-1):\n",
    "                    sentence_abbreviation = Abstract_wiki[5:Abstract_wiki.find(' ('+word_a+')')]\n",
    "                else:\n",
    "                    if(Abstract_wiki.find(word_a+' (')!=-1):\n",
    "                        start_position = Abstract_wiki.find(word_a+' (')+len(word_a)+2\n",
    "                        end_position = Abstract_wiki[start_position:].find(')')\n",
    "                        sentence_abbreviation = Abstract_wiki[start_position:start_position+end_position]\n",
    "                        if(open_or_not):\n",
    "                            print(sentence_abbreviation)\n",
    "                    else:\n",
    "                        sentence_abbreviation = Abstract_wiki[5:Abstract_wiki.find(' ('+word_a+')')]\n",
    "            else:\n",
    "                if(Abstract_wiki.find(' ('+word_a+')')!=-1):\n",
    "                    sentence_abbreviation = Abstract_wiki[:Abstract_wiki.find(' ('+word_a+')')]\n",
    "                else:\n",
    "                    if(Abstract_wiki.find(word_a+' (')!=-1):\n",
    "                        start_position = Abstract_wiki.find(word_a+' (')+len(word_a)+2\n",
    "                        end_position = Abstract_wiki[start_position:].find(')')\n",
    "                        sentence_abbreviation = Abstract_wiki[start_position:start_position+end_position]\n",
    "                        if(open_or_not):\n",
    "                            print(sentence_abbreviation)\n",
    "                    else:\n",
    "                        sentence_abbreviation = Abstract_wiki[:Abstract_wiki.find(' ('+word_a+')')]\n",
    "\n",
    "\n",
    "            if ( sentence_abbreviation.find('\\n') != -1 ):\n",
    "            #     print(sentence_abbreviation.find('\\n'))\n",
    "                if(open_or_not):\n",
    "                    print('有換行')\n",
    "                sentence_abbreviation = sentence_abbreviation[:sentence_abbreviation.find('\\n')]\n",
    "                if(open_or_not):\n",
    "                    print('-----------')\n",
    "                    print(sentence_abbreviation)\n",
    "            else:\n",
    "                if(open_or_not):\n",
    "                    print('-----------')\n",
    "                    print(sentence_abbreviation)\n",
    "        else:\n",
    "            sentence_abbreviation = ''\n",
    "\n",
    "        return sentence_abbreviation\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus & acid距離 : \n",
      "0.04287189245223999\n"
     ]
    }
   ],
   "source": [
    "word_a = 'virus'\n",
    "word_b = 'acid'\n",
    "\n",
    "two_word_distance = model.wv.distance(word_a,word_b)\n",
    "print(word_a+' & '+word_b+'距離 : ')\n",
    "print(two_word_distance)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "縮寫字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可能之縮寫字 :  519\n"
     ]
    }
   ],
   "source": [
    "print('可能之縮寫字 : ',len(Dictionary_abbreviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FMDV', 'RNA', 'PCR', 'VP1', 'HERV-K', 'LTR', 'HCV', 'DNA', 'CV-N', 'HIV-1']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionary_abbreviation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_domain = ['nucleic','PCR','virus'] #,'noninfectious'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMDV & nucleic距離 : \n",
      "0.313747763633728\n",
      "\n",
      "FMDV & PCR距離 : \n",
      "0.12094348669052124\n",
      "\n",
      "FMDV & virus距離 : \n",
      "0.10767906904220581\n",
      "\n",
      "搜尋目標 :  virus\n",
      "第二搜尋目標 :  PCR\n"
     ]
    }
   ],
   "source": [
    "word_a = 'FMDV'\n",
    "\n",
    "dis = []\n",
    "for each_domain in want_domain:\n",
    "    word_b = each_domain\n",
    "\n",
    "    two_word_distance = model.wv.distance(word_a,word_b)\n",
    "    print(word_a+' & '+word_b+'距離 : ')\n",
    "    print(two_word_distance)\n",
    "    dis.append(two_word_distance)\n",
    "    print()\n",
    "\n",
    "Domain = want_domain[dis.index(min(dis))]\n",
    "\n",
    "sec_min = 100\n",
    "sec_min_position = 0\n",
    "\n",
    "pos = 0\n",
    "for each_dis in dis :\n",
    "    if((each_dis-min(dis))!=0 and each_dis<sec_min):\n",
    "        sec_min = each_dis\n",
    "        Sec_Domain = want_domain[pos]\n",
    "    pos += 1\n",
    "\n",
    "print('搜尋目標 : ', Domain)\n",
    "print('第二搜尋目標 : ', Sec_Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Foot-and-mouth disease virus'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search(word_a,Domain) #,True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "google_search(word_a,Sec_Domain) #,True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hepatitis C is an infectious disease caused by the hepatitis C virus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: use options instead of chrome_options\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "chrome_options=Options()\n",
    "chrome_options.add_argument('--headless')#無介面操作\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "want_domain = ['nucleic','PCR','virus']\n",
    "Dictionary_abbreviation_final = []\n",
    "\n",
    "strange_word = '網路上的精選摘要全部顯示翻譯這個網頁'\n",
    "\n",
    "for seach_target in Dictionary_abbreviation[:10] :\n",
    "    word_a = seach_target\n",
    "\n",
    "    dis = []\n",
    "    for each_domain in want_domain:\n",
    "        word_b = each_domain\n",
    "        two_word_distance = model.wv.distance(word_a,word_b)\n",
    "        dis.append(two_word_distance)\n",
    "\n",
    "    Domain = want_domain[dis.index(min(dis))] \n",
    "    sec_min = 100\n",
    "    sec_min_position = 0\n",
    "\n",
    "    pos = 0\n",
    "    for each_dis in dis :\n",
    "        if((each_dis-min(dis))!=0 and each_dis<sec_min):\n",
    "            sec_min = each_dis\n",
    "            Sec_Domain = want_domain[pos]\n",
    "        pos += 1\n",
    "\n",
    "    Full_name = google_search(word_a,Domain)\n",
    "    if(Full_name==''):\n",
    "        Full_name = google_search(word_a,Sec_Domain)\n",
    "    for strange in strange_word :\n",
    "        if(strange in Full_name):\n",
    "            Full_name = google_search(word_a,Sec_Domain)\n",
    "            break\n",
    "    if(Full_name==''):\n",
    "        Full_name = google_search(word_a,'')\n",
    "    Dictionary_abbreviation_final.append([word_a,Full_name])\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FMDV', 'Foot-and-mouth disease virus'],\n",
       " ['RNA', 'ribonucleic acid'],\n",
       " ['PCR', 'Polymerase chain reaction'],\n",
       " ['VP1', 'Major capsid protein'],\n",
       " ['HERV-K', 'human endogenous retrovirus-K'],\n",
       " ['LTR', 'long terminal repeat'],\n",
       " ['HCV', 'Virology. The hepatitis C virus'],\n",
       " ['DNA', 'Polymerase chain reaction'],\n",
       " ['CV-N', 'curriculum vitae'],\n",
       " ['HIV-1', 'The human immunodeficiency virus type 1']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionary_abbreviation_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 挑選成NE字典\n",
    "#### (在意名詞之間關係排序)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_texts_n_NE = word_texts_n[:1000]\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_n_sort_dis = []\n",
    "index_name = []\n",
    "for each_n in word_texts_n_NE :\n",
    "    sort_word = model.wv.most_similar(positive=[each_n], negative=[], topn=len(word_texts_n)+len(word_texts_v))\n",
    "    list_n_sort_dis_this = []\n",
    "    for each_distance in sort_word :\n",
    "        #高於threshold\n",
    "        if(each_distance[1]>=threshold):\n",
    "            #在\"NE\"內\n",
    "            if each_distance[0] in word_texts_n_NE:\n",
    "                list_n_sort_dis_this.append(each_distance)\n",
    "    list_n_sort_dis.append(list_n_sort_dis_this)\n",
    "    index_name.append(each_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary_word2vec = DataFrame(list_n_sort_dis,index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          (virus, 0.9849257469177246)\n",
       "1       (sequence, 0.9782947301864624)\n",
       "2            (gene, 0.977288007736206)\n",
       "3      (infection, 0.9770529866218567)\n",
       "4            (RNA, 0.9758744239807129)\n",
       "                    ...               \n",
       "313    (objective, 0.5020682215690613)\n",
       "314     (research, 0.5005376935005188)\n",
       "315                               None\n",
       "316                               None\n",
       "317                               None\n",
       "Name: DNA, Length: 318, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionary_word2vec.loc[\"DNA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HearstPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_H_set = []\n",
    "D_H_set_w = []\n",
    "for First_w in range(len(Dictionary_HearstPatterns_word)) :\n",
    "    if Dictionary_HearstPatterns_word[First_w][0] not in D_H_set_w :\n",
    "        D_H_set.append([[Dictionary_HearstPatterns_word[First_w][1]],[Dictionary_HearstPatterns_time[First_w]]])\n",
    "        D_H_set_w.append(Dictionary_HearstPatterns_word[First_w][0])\n",
    "    else:\n",
    "        position = D_H_set_w.index(Dictionary_HearstPatterns_word[First_w][0])\n",
    "        D_H_set[position][0].append(Dictionary_HearstPatterns_word[First_w][1])\n",
    "        D_H_set[position][1].append(Dictionary_HearstPatterns_time[First_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionart_HearstPatterns = DataFrame(D_H_set,index=D_H_set_w)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "建立 所有表示法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_H_set = []\n",
    "D_H_set_w = []\n",
    "for First_w in range(len(Dictionary_HearstPatterns_word)) :\n",
    "    if Dictionary_HearstPatterns_word[First_w][0] not in D_H_set_w :\n",
    "        D_H_set.append([[Dictionary_HearstPatterns_word[First_w][1]],[Dictionary_HearstPatterns_time[First_w]]])\n",
    "        D_H_set_w.append(Dictionary_HearstPatterns_word[First_w][0])\n",
    "    else:\n",
    "        position = D_H_set_w.index(Dictionary_HearstPatterns_word[First_w][0])\n",
    "        D_H_set[position][0].append(Dictionary_HearstPatterns_word[First_w][1])\n",
    "        D_H_set[position][1].append(Dictionary_HearstPatterns_time[First_w])\n",
    "\n",
    "Dictionart_HearstPatterns = DataFrame(D_H_set,index=D_H_set_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the cell growth  promote factor</th>\n",
       "      <td>[protein]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RIV</th>\n",
       "      <td>[a putative HIV vaccine]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CS mutation</th>\n",
       "      <td>[the clinical isolate]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a high sensitivity</th>\n",
       "      <td>[the EM analysis]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some virus</th>\n",
       "      <td>[microbe]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the Enhanced Green Fluorescent Protein</th>\n",
       "      <td>[a marker]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chromosomal analysis</th>\n",
       "      <td>[further study]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug treatment complication</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radiation effect</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recurrent tumor</th>\n",
       "      <td>[noninfectious entity]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0    1\n",
       "the cell growth  promote factor                        [protein]  [2]\n",
       "RIV                                     [a putative HIV vaccine]  [1]\n",
       "CS mutation                               [the clinical isolate]  [1]\n",
       "a high sensitivity                             [the EM analysis]  [1]\n",
       "some virus                                             [microbe]  [1]\n",
       "the Enhanced Green Fluorescent Protein                [a marker]  [1]\n",
       "chromosomal analysis                             [further study]  [1]\n",
       "drug treatment complication               [noninfectious entity]  [1]\n",
       "radiation effect                          [noninfectious entity]  [1]\n",
       "recurrent tumor                           [noninfectious entity]  [1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionart_HearstPatterns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [protein]\n",
       "1          [2]\n",
       "Name: the cell growth  promote factor, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionart_HearstPatterns.loc[\"the cell growth  promote factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a putative HIV vaccine]\n",
       "1                         [1]\n",
       "Name: RIV, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionart_HearstPatterns.loc[\"RIV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 屬性字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attributes=[\n",
    "'is known as',\n",
    "'is composed of',\n",
    "'is a kind of',\n",
    "'is a',\n",
    "'is',\n",
    "'are known as',\n",
    "'are composed of',\n",
    "'are a kind of',\n",
    "'are',\n",
    "'called',\n",
    "'belong'\n",
    "'is belong',\n",
    "'isolated from',\n",
    "'isolates',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus is unimpaired\n",
      "\n",
      "virus is non-infectious in transfected cell culture and in injected sheep\n",
      "\n",
      "virus are common in infancy, causing mostly asymptomatic infections\n",
      "\n",
      "virus isolated from China (TMV-Cv)\n",
      "\n",
      "virus isolates were identified by neutralization, immunofluorescence assay, or enzyme immunoassay\n",
      "\n",
      "virus are important to curtail the spread of this virus\n",
      "\n",
      "virus is the most important foodborne virus in Japan\n",
      "\n",
      "virus are increasingly recognized worldwide as the most important cause of food borne gastro-intestinal illness\n",
      "\n",
      "virus is probably the main causative agent of Fuchs uveitis, but other viruses may also be involved in the pathogenesis of this disease\n",
      "\n",
      "virus is commonly found in environmental waters and is very resistant to water disinfection and environmental stressors, especially UV light inactivation\n",
      "\n",
      "virus is regarded as the major risk factor in the development of cervical cancer\n",
      "\n",
      "virus is mainly maintained through human-mosquito-human cycle\n",
      "\n",
      "virus is a globally spread zoonotic arbovirus\n",
      "\n",
      "virus is recognised as a significant food safety risk\n",
      "\n",
      "virus is primarily transmitted from person to person through inhalation of infected droplets or aerosols\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Search_Attributes_target = 'virus' # virus DNA RNA\n",
    "\n",
    "a = 0\n",
    "\n",
    "for each_prepose_paper in raw_papers :\n",
    "    a+=1\n",
    "    text = each_prepose_paper\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for each_sentence in sentences :\n",
    "        for Attributes_situation in Attributes :\n",
    "            target_position = each_sentence.find(Search_Attributes_target+' '+Attributes_situation+' ')\n",
    "            if(target_position!=-1):\n",
    "                \n",
    "                Attributes_Sentence = each_sentence[target_position:target_position+each_sentence[target_position:].find('.')]\n",
    "                Attributes_Sentence2 = Attributes_Sentence.split()\n",
    "                Attributes_Sentence_target =  len(Attributes_Sentence2)-len(each_sentence[len(Search_Attributes_target+' '+Attributes_situation)+target_position+1 :].split())\n",
    "\n",
    "                tagged = pos_tag(Attributes_Sentence2)\n",
    "                case = []\n",
    "                for each_word, pos in tagged[Attributes_Sentence_target:] :\n",
    "                    \n",
    "                    Part_of_speech = is_Part_of_speech(pos)\n",
    "                    case.append(Part_of_speech)\n",
    "#                     print(each_word,Part_of_speech)\n",
    "                if('adjective' in case or 'noun' in case):\n",
    "                    print(Attributes_Sentence)\n",
    "                    print()\n",
    "                break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "UMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: use options instead of chrome_options\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "chrome_options=Options()\n",
    "chrome_options.add_argument('--headless')#無介面操作\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "want_domain = ['nucleic','PCR','virus']\n",
    "Dictionary_abbreviation_final = []\n",
    "\n",
    "strange_word = '網路上的精選摘要全部顯示翻譯這個網頁'\n",
    "\n",
    "for seach_target in [Search_Attributes_target] :\n",
    "    word_a = seach_target\n",
    "\n",
    "    dis = []\n",
    "    for each_domain in want_domain:\n",
    "        word_b = each_domain\n",
    "        two_word_distance = model.wv.distance(word_a,word_b)\n",
    "        dis.append(two_word_distance)\n",
    "\n",
    "    Domain = want_domain[dis.index(min(dis))] \n",
    "    sec_min = 100\n",
    "    sec_min_position = 0\n",
    "\n",
    "    pos = 0\n",
    "    for each_dis in dis :\n",
    "        if((each_dis-min(dis))!=0 and each_dis<sec_min):\n",
    "            sec_min = each_dis\n",
    "            Sec_Domain = want_domain[pos]\n",
    "        pos += 1\n",
    "\n",
    "    Full_name = google_search(word_a,Domain)\n",
    "    if(Full_name==''):\n",
    "        Full_name = google_search(word_a,Sec_Domain)\n",
    "    for strange in strange_word :\n",
    "        if(strange in Full_name):\n",
    "            Full_name = google_search(word_a,Sec_Domain)\n",
    "            break\n",
    "    if(Full_name==''):\n",
    "        Full_name = google_search(word_a,'')\n",
    "    Dictionary_abbreviation_final.append([word_a,Full_name])\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 經過 POS Tagging(4-type) 、 詞形還原 、 lower 篩選過後的論文\n",
    "\n",
    "##### 進入第二遍之前還需要看過斷詞字典、縮寫字典\n",
    "##### 統計詞頻前還需確認是否不在stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'removal virus infectivity blood biopharmaceutical product prepare blood issue considerable importance . irrespective choose vital biological activity product impaired . blood unfractionated plasma serum problem challenging . inactivation genome key kill virus vaccines . imines inactivated foot-and-mouth virus vaccine evidence survival virus infectivity . immunogenicity virus unimpaired . viruses belong recognised inactivate imines . biological property proteins cell growth-promoting factor calf serum impaired condition ensure inactivation infectious poliovirus foot-and-mouth virus FMDV . virus inactivate imines degree provide remove infectivity protein unstable temperatures . mechanism FMDV inactivate studied . find RNA extract virus inactivation degree degrade hidden break non-infectious . amplify PCR primer correspond gene cod portion viral RNA polymerase cod VP1 structural proteins alteration base base occur . '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
